{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Project Part 2\n","\n","[![Kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://kaggle.com/kernels/welcome?src=https://github.com/entrylevelcs/CS39AA-Project/blob/main/project_part2.ipynb)\n","\n","[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/entrylevelcs/CS39AA-Project/blob/main/project_part2.ipynb)\n"]},{"cell_type":"markdown","metadata":{},"source":["## 1. Introduction/Background\n","\n","For this part of the project we are taking the nearest neighbors model and training it using both our AI and real data sets. From this simple model we can see how well each of the different training data sets do after being optimizing the hyperparameters of this simple model."]},{"cell_type":"markdown","metadata":{},"source":["## 2. Simple Modeling with KNeighborsClassifier"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["c:\\Users\\Muajeh Lee\\anaconda3\\Lib\\site-packages\\transformers\\utils\\generic.py:260: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n","  torch.utils._pytree._register_pytree_node(\n","c:\\Users\\Muajeh Lee\\anaconda3\\Lib\\site-packages\\transformers\\utils\\generic.py:260: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n","  torch.utils._pytree._register_pytree_node(\n"]}],"source":["# import all of the python modules/packages you'll need here\n","import pandas as pd\n","import numpy as np\n","from sklearn.model_selection import train_test_split\n","import matplotlib.pyplot as plt\n","import torch\n","from transformers import AutoTokenizer, GPT2ForSequenceClassification,  TrainingArguments, Trainer\n","from datasets import Dataset, load_metric\n","# ..."]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Get the data set from real steam reviews. The specific set of data that I am using for this notebook comes from https://www.kaggle.com/datasets/andrewmvd/steam-reviews/ but is just a sample of 25000 from the entire set."]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["human_data = 'https://raw.githubusercontent.com/entrylevelcs/CS39AA-Project/main/human_dataset.csv'\n","df = pd.read_csv(human_data)\n","df = df[df[\"review_text\"].notnull()]\n","sample_size = len(df)\n","#sample_size = 25000\n","#df = df.sample(sample_size)\n","# previous two lines are for sampling from the entire human dataset"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Unnamed: 0</th>\n","      <th>app_id</th>\n","      <th>app_name</th>\n","      <th>review_text</th>\n","      <th>review_score</th>\n","      <th>review_votes</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>949219</td>\n","      <td>212680</td>\n","      <td>FTL: Faster Than Light</td>\n","      <td>Really gg.</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1151136</td>\n","      <td>218620</td>\n","      <td>PAYDAY 2</td>\n","      <td>I'm not entirely sure what I was expecting, bu...</td>\n","      <td>-1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>4031479</td>\n","      <td>31280</td>\n","      <td>Poker Night at the Inventory</td>\n","      <td>First I thought I was gonna buy this, play so ...</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>6382952</td>\n","      <td>94400</td>\n","      <td>Nidhogg</td>\n","      <td>Stab your opponent repeatedly and win the glor...</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>3770644</td>\n","      <td>300060</td>\n","      <td>ADR1FT</td>\n","      <td>I think this experience is pretty much defined...</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   Unnamed: 0  app_id                      app_name  \\\n","0      949219  212680        FTL: Faster Than Light   \n","1     1151136  218620                      PAYDAY 2   \n","2     4031479   31280  Poker Night at the Inventory   \n","3     6382952   94400                       Nidhogg   \n","4     3770644  300060                        ADR1FT   \n","\n","                                         review_text  review_score  \\\n","0                                         Really gg.             1   \n","1  I'm not entirely sure what I was expecting, bu...            -1   \n","2  First I thought I was gonna buy this, play so ...             1   \n","3  Stab your opponent repeatedly and win the glor...             1   \n","4  I think this experience is pretty much defined...             1   \n","\n","   review_votes  \n","0             0  \n","1             0  \n","2             0  \n","3             0  \n","4             0  "]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["df.head()"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Get the data set that was generated by chatgpt. This review data started as only being about CS:GO but has been expanded to be more general and talk about other games."]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["ai_data = 'https://raw.githubusercontent.com/entrylevelcs/CS39AA-Project/main/gpt3.5_generated_data.csv'\n","df2 = pd.read_csv(ai_data)"]},{"cell_type":"markdown","metadata":{},"source":["Sets up our different inputs and outputs. X and y are the inputs and outputs from the real steam reviews while X1 and y1 are from the AI generated reviews. I also adjusted the test size so that the training data size the same length as the AI generated data set. From the shape of each we can see that the vocab size is 11127 words."]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["ai_data_size = df2.size\n","sample_size = df.size\n","proportion = ((sample_size-ai_data_size)/sample_size)"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["c:\\Users\\Muajeh Lee\\anaconda3\\Lib\\site-packages\\pyarrow\\pandas_compat.py:373: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n","  if _pandas_api.is_sparse(col):\n","Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}],"source":["real_data_raw = Dataset.from_pandas(df[['review_text', 'review_score']])\n","simulated_data_raw = Dataset.from_pandas(df2)\n","real_tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n","simulated_tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n","realModel = GPT2ForSequenceClassification.from_pretrained(\"gpt2\")\n","simulatedModel = GPT2ForSequenceClassification.from_pretrained(\"gpt2\")\n"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"beef052908444860ac6be20a190d6517","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/24971 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f6ff9a677de14cf49e00f2fdc0924762","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/2496 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"}],"source":["def tokenize_function(examples):\n","    return real_tokenizer(examples[\"review_text\"], truncation=True, max_length=55)\n","def tokenize_function2(examples):\n","    return simulated_tokenizer(examples[\"Review\"], truncation=True, max_length=55)\n","\n","real_data = real_data_raw.map(tokenize_function, batched=True)\n","simulated_data = simulated_data_raw.map(tokenize_function2, batched=True)"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["train_prop = proportion\n","real_data_train = real_data.select(range(int(len(real_data)*train_prop)))\n","real_data_eval = real_data.select(range(int(len(real_data)*train_prop), len(real_data)))"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["C:\\Users\\Muajeh Lee\\AppData\\Local\\Temp\\ipykernel_10632\\3120487532.py:1: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate\n","  metric = load_metric(\"accuracy\")\n"]}],"source":["metric = load_metric(\"accuracy\")\n","\n","def compute_metrics(eval_pred):\n","    logits, labels = eval_pred\n","    predictions = np.argmax(logits, axis=-1)\n","    return metric.compute(predictions=predictions, references=labels)\n","\n","training_args = TrainingArguments(num_train_epochs=10,\n","                                  do_train=True,\n","                                  report_to=None,\n","                                  output_dir=\"real\",\n","                                  evaluation_strategy=\"steps\",\n","                                  eval_steps=200,\n","                                  learning_rate=1e-5,\n","                                  per_device_train_batch_size=32,\n","                                  per_device_eval_batch_size=32)\n","\n","trainer = Trainer(model = realModel, \n","                  args = training_args,\n","                  train_dataset = real_data_train, \n","                  eval_dataset = real_data_eval,\n","                  compute_metrics = compute_metrics,\n",")"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Using GPU\n"]}],"source":["if torch.cuda.is_available():\n","    device = \"cuda:0\"\n","    print(\"Using GPU\")\n","else: \n","    device = \"cpu\"\n"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[{"data":{"text/plain":["GPT2ForSequenceClassification(\n","  (transformer): GPT2Model(\n","    (wte): Embedding(50257, 768)\n","    (wpe): Embedding(1024, 768)\n","    (drop): Dropout(p=0.1, inplace=False)\n","    (h): ModuleList(\n","      (0-11): 12 x GPT2Block(\n","        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","        (attn): GPT2Attention(\n","          (c_attn): Conv1D()\n","          (c_proj): Conv1D()\n","          (attn_dropout): Dropout(p=0.1, inplace=False)\n","          (resid_dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","        (mlp): GPT2MLP(\n","          (c_fc): Conv1D()\n","          (c_proj): Conv1D()\n","          (act): NewGELUActivation()\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","    )\n","    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","  )\n","  (score): Linear(in_features=768, out_features=2, bias=False)\n",")"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["realModel.to(device)"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"4307ecc9a5a94a8f86e901e0d9bf7ab3","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/7550 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"ename":"ValueError","evalue":"expected sequence of length 3 at dim 1 (got 27)","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[1;32mm:\\Documents\\NLP\\project\\CS39AA-Project\\project_part3.ipynb Cell 18\u001b[0m line \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/m%3A/Documents/NLP/project/CS39AA-Project/project_part3.ipynb#X23sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m torch\u001b[39m.\u001b[39mset_grad_enabled(\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/m%3A/Documents/NLP/project/CS39AA-Project/project_part3.ipynb#X23sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m trainer\u001b[39m.\u001b[39mtrain()\n\u001b[0;32m      <a href='vscode-notebook-cell:/m%3A/Documents/NLP/project/CS39AA-Project/project_part3.ipynb#X23sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m trainer\u001b[39m.\u001b[39mevaluate()\n","File \u001b[1;32mc:\\Users\\Muajeh Lee\\anaconda3\\Lib\\site-packages\\transformers\\trainer.py:1555\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1553\u001b[0m         hf_hub_utils\u001b[39m.\u001b[39menable_progress_bars()\n\u001b[0;32m   1554\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1555\u001b[0m     \u001b[39mreturn\u001b[39;00m inner_training_loop(\n\u001b[0;32m   1556\u001b[0m         args\u001b[39m=\u001b[39margs,\n\u001b[0;32m   1557\u001b[0m         resume_from_checkpoint\u001b[39m=\u001b[39mresume_from_checkpoint,\n\u001b[0;32m   1558\u001b[0m         trial\u001b[39m=\u001b[39mtrial,\n\u001b[0;32m   1559\u001b[0m         ignore_keys_for_eval\u001b[39m=\u001b[39mignore_keys_for_eval,\n\u001b[0;32m   1560\u001b[0m     )\n","File \u001b[1;32mc:\\Users\\Muajeh Lee\\anaconda3\\Lib\\site-packages\\transformers\\trainer.py:1815\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   1812\u001b[0m     rng_to_sync \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m   1814\u001b[0m step \u001b[39m=\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m\n\u001b[1;32m-> 1815\u001b[0m \u001b[39mfor\u001b[39;00m step, inputs \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(epoch_iterator):\n\u001b[0;32m   1816\u001b[0m     total_batched_samples \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m   1817\u001b[0m     \u001b[39mif\u001b[39;00m rng_to_sync:\n","File \u001b[1;32mc:\\Users\\Muajeh Lee\\anaconda3\\Lib\\site-packages\\accelerate\\data_loader.py:448\u001b[0m, in \u001b[0;36mDataLoaderShard.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    446\u001b[0m \u001b[39m# We iterate one batch ahead to check when we are at the end\u001b[39;00m\n\u001b[0;32m    447\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 448\u001b[0m     current_batch \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39m(dataloader_iter)\n\u001b[0;32m    449\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m:\n\u001b[0;32m    450\u001b[0m     \u001b[39myield\u001b[39;00m\n","File \u001b[1;32mc:\\Users\\Muajeh Lee\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    628\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    629\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    630\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 631\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_data()\n\u001b[0;32m    632\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    633\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    635\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n","File \u001b[1;32mc:\\Users\\Muajeh Lee\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    673\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    674\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 675\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_fetcher\u001b[39m.\u001b[39mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    676\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[0;32m    677\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n","File \u001b[1;32mc:\\Users\\Muajeh Lee\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:54\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n\u001b[1;32m---> 54\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcollate_fn(data)\n","File \u001b[1;32mc:\\Users\\Muajeh Lee\\anaconda3\\Lib\\site-packages\\transformers\\data\\data_collator.py:70\u001b[0m, in \u001b[0;36mdefault_data_collator\u001b[1;34m(features, return_tensors)\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[39m# In this function we'll make the assumption that all `features` in the batch\u001b[39;00m\n\u001b[0;32m     65\u001b[0m \u001b[39m# have the same attributes.\u001b[39;00m\n\u001b[0;32m     66\u001b[0m \u001b[39m# So we will look at the first element as a proxy for what attributes exist\u001b[39;00m\n\u001b[0;32m     67\u001b[0m \u001b[39m# on the whole batch.\u001b[39;00m\n\u001b[0;32m     69\u001b[0m \u001b[39mif\u001b[39;00m return_tensors \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m---> 70\u001b[0m     \u001b[39mreturn\u001b[39;00m torch_default_data_collator(features)\n\u001b[0;32m     71\u001b[0m \u001b[39melif\u001b[39;00m return_tensors \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mtf\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m     72\u001b[0m     \u001b[39mreturn\u001b[39;00m tf_default_data_collator(features)\n","File \u001b[1;32mc:\\Users\\Muajeh Lee\\anaconda3\\Lib\\site-packages\\transformers\\data\\data_collator.py:136\u001b[0m, in \u001b[0;36mtorch_default_data_collator\u001b[1;34m(features)\u001b[0m\n\u001b[0;32m    134\u001b[0m             batch[k] \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(np\u001b[39m.\u001b[39mstack([f[k] \u001b[39mfor\u001b[39;00m f \u001b[39min\u001b[39;00m features]))\n\u001b[0;32m    135\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 136\u001b[0m             batch[k] \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor([f[k] \u001b[39mfor\u001b[39;00m f \u001b[39min\u001b[39;00m features])\n\u001b[0;32m    138\u001b[0m \u001b[39mreturn\u001b[39;00m batch\n","\u001b[1;31mValueError\u001b[0m: expected sequence of length 3 at dim 1 (got 27)"]}],"source":["torch.set_grad_enabled(True)\n","trainer.train()\n","trainer.evaluate()"]}],"metadata":{"kernelspec":{"display_name":"Python 3.9.12 ('base')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.6"},"orig_nbformat":4,"vscode":{"interpreter":{"hash":"bb3398f4b21c7b026dd5874af3f954bf25f1e8ff81e25d82a94abcbbaacf760b"}}},"nbformat":4,"nbformat_minor":2}
