{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Project Part 2\n","\n","[![Kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://kaggle.com/kernels/welcome?src=https://github.com/entrylevelcs/CS39AA-Project/blob/main/project_part3.ipynb)\n","\n","[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/entrylevelcs/CS39AA-Project/blob/main/project_part3.ipynb)\n"]},{"cell_type":"markdown","metadata":{},"source":["## 1. Introduction/Background\n","\n","For this part of the project we are taking the nearest neighbors model and training it using both our AI and real data sets. From this simple model we can see how well each of the different training data sets do after being optimizing the hyperparameters of this simple model."]},{"cell_type":"markdown","metadata":{},"source":["## 2. Simple Modeling with KNeighborsClassifier"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["c:\\Users\\Muajeh Lee\\anaconda3\\Lib\\site-packages\\transformers\\utils\\generic.py:260: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n","  torch.utils._pytree._register_pytree_node(\n","c:\\Users\\Muajeh Lee\\anaconda3\\Lib\\site-packages\\transformers\\utils\\generic.py:260: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n","  torch.utils._pytree._register_pytree_node(\n"]}],"source":["# import all of the python modules/packages you'll need here\n","import pandas as pd\n","import numpy as np\n","import torch\n","from transformers import AutoTokenizer, AutoModelForSequenceClassification,  TrainingArguments, Trainer\n","from datasets import Dataset, load_metric\n","# ..."]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Get the data set from real steam reviews. The specific set of data that I am using for this notebook comes from https://www.kaggle.com/datasets/andrewmvd/steam-reviews/ but is just a sample of 25000 from the entire set."]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["human_data = 'https://raw.githubusercontent.com/entrylevelcs/CS39AA-Project/main/human_dataset.csv'\n","df = pd.read_csv(human_data)\n","df = df[df[\"review_text\"].notnull()]\n","sample_size = len(df)\n","#sample_size = 25000\n","#df = df.sample(sample_size)\n","# previous two lines are for sampling from the entire human dataset"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Get the data set that was generated by chatgpt. This review data started as only being about CS:GO but has been expanded to be more general and talk about other games."]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["ai_data = 'https://raw.githubusercontent.com/entrylevelcs/CS39AA-Project/main/gpt3.5_generated_data.csv'\n","df2 = pd.read_csv(ai_data)"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["df = df.rename(columns={\"review_text\": \"text\", \"review_score\": \"label\"})\n","df2 = df2.rename(columns={\"Review\": \"text\", \" Sentiment\": \"label\"})"]},{"cell_type":"markdown","metadata":{},"source":["Sets up our different inputs and outputs. X and y are the inputs and outputs from the real steam reviews while X1 and y1 are from the AI generated reviews. I also adjusted the test size so that the training data size the same length as the AI generated data set. From the shape of each we can see that the vocab size is 11127 words."]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["ai_data_size = df2.size\n","sample_size = df.size\n","proportion = ((sample_size-ai_data_size)/sample_size)"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["c:\\Users\\Muajeh Lee\\anaconda3\\Lib\\site-packages\\pyarrow\\pandas_compat.py:373: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n","  if _pandas_api.is_sparse(col):\n","c:\\Users\\Muajeh Lee\\anaconda3\\Lib\\site-packages\\transformers\\utils\\generic.py:260: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n","  torch.utils._pytree._register_pytree_node(\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}],"source":["real_data_raw = Dataset.from_pandas(df[['text', 'label']])\n","simulated_data_raw = Dataset.from_pandas(df2)\n","#MODEL_NAME = 'gpt2'\n","MODEL_NAME = 'bert-base-cased'\n","real_tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n","simulated_tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n","realModel = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=2, max_length=55, output_attentions=False, output_hidden_states=False)\n","simulatedModel = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=2, max_length=55, output_attentions=False, output_hidden_states=False)\n"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b36b667bc83a469a8364599c0fa517f5","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/24971 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"4349a5493e77473a9d65c0226d1ae2d1","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/2496 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"}],"source":["def tokenize_function(examples):\n","    return real_tokenizer(examples[\"text\"], padding='max_length', truncation=True, max_length=55)\n","\n","real_data = real_data_raw.map(tokenize_function, batched=True)\n","simulated_data = simulated_data_raw.map(tokenize_function, batched=True)"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["train_prop = proportion\n","real_data_train = real_data.select(range(int(len(real_data)*train_prop)))\n","real_data_eval = real_data.select(range(int(len(real_data)*train_prop), len(real_data)))"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["C:\\Users\\Muajeh Lee\\AppData\\Local\\Temp\\ipykernel_10808\\3120487532.py:1: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate\n","  metric = load_metric(\"accuracy\")\n"]}],"source":["metric = load_metric(\"accuracy\")\n","\n","def compute_metrics(eval_pred):\n","    logits, labels = eval_pred\n","    predictions = np.argmax(logits, axis=-1)\n","    return metric.compute(predictions=predictions, references=labels)\n","\n","training_args = TrainingArguments(num_train_epochs=10,\n","                                  do_train=True,\n","                                  report_to=None,\n","                                  output_dir=\"real\",\n","                                  evaluation_strategy=\"steps\",\n","                                  eval_steps=200,\n","                                  learning_rate=1e-5,\n","                                  per_device_train_batch_size=32,\n","                                  per_device_eval_batch_size=32)\n","\n","trainer = Trainer(model = realModel, \n","                  args = training_args,\n","                  train_dataset = real_data_train, \n","                  eval_dataset = real_data_eval,\n","                  compute_metrics = compute_metrics,\n",")"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Using GPU\n"]}],"source":["if torch.cuda.is_available():\n","    device = \"cuda:0\"\n","    print(\"Using GPU\")\n","else: \n","    device = \"cpu\"\n"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[{"data":{"text/plain":["BertForSequenceClassification(\n","  (bert): BertModel(\n","    (embeddings): BertEmbeddings(\n","      (word_embeddings): Embedding(28996, 768, padding_idx=0)\n","      (position_embeddings): Embedding(512, 768)\n","      (token_type_embeddings): Embedding(2, 768)\n","      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (encoder): BertEncoder(\n","      (layer): ModuleList(\n","        (0-11): 12 x BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","    (pooler): BertPooler(\n","      (dense): Linear(in_features=768, out_features=768, bias=True)\n","      (activation): Tanh()\n","    )\n","  )\n","  (dropout): Dropout(p=0.1, inplace=False)\n","  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",")"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["realModel.to(device)"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e8543924e4f24b87854920e26e7d0f30","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/7550 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"ename":"RuntimeError","evalue":"CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[1;32mm:\\Documents\\NLP\\project\\CS39AA-Project\\project_part3.ipynb Cell 18\u001b[0m line \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/m%3A/Documents/NLP/project/CS39AA-Project/project_part3.ipynb#X23sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m torch\u001b[39m.\u001b[39mset_grad_enabled(\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/m%3A/Documents/NLP/project/CS39AA-Project/project_part3.ipynb#X23sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m trainer\u001b[39m.\u001b[39mtrain()\n\u001b[0;32m      <a href='vscode-notebook-cell:/m%3A/Documents/NLP/project/CS39AA-Project/project_part3.ipynb#X23sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m trainer\u001b[39m.\u001b[39mevaluate()\n","File \u001b[1;32mc:\\Users\\Muajeh Lee\\anaconda3\\Lib\\site-packages\\transformers\\trainer.py:1555\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1553\u001b[0m         hf_hub_utils\u001b[39m.\u001b[39menable_progress_bars()\n\u001b[0;32m   1554\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1555\u001b[0m     \u001b[39mreturn\u001b[39;00m inner_training_loop(\n\u001b[0;32m   1556\u001b[0m         args\u001b[39m=\u001b[39margs,\n\u001b[0;32m   1557\u001b[0m         resume_from_checkpoint\u001b[39m=\u001b[39mresume_from_checkpoint,\n\u001b[0;32m   1558\u001b[0m         trial\u001b[39m=\u001b[39mtrial,\n\u001b[0;32m   1559\u001b[0m         ignore_keys_for_eval\u001b[39m=\u001b[39mignore_keys_for_eval,\n\u001b[0;32m   1560\u001b[0m     )\n","File \u001b[1;32mc:\\Users\\Muajeh Lee\\anaconda3\\Lib\\site-packages\\transformers\\trainer.py:1837\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   1834\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_handler\u001b[39m.\u001b[39mon_step_begin(args, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol)\n\u001b[0;32m   1836\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maccelerator\u001b[39m.\u001b[39maccumulate(model):\n\u001b[1;32m-> 1837\u001b[0m     tr_loss_step \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining_step(model, inputs)\n\u001b[0;32m   1839\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[0;32m   1840\u001b[0m     args\u001b[39m.\u001b[39mlogging_nan_inf_filter\n\u001b[0;32m   1841\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[0;32m   1842\u001b[0m     \u001b[39mand\u001b[39;00m (torch\u001b[39m.\u001b[39misnan(tr_loss_step) \u001b[39mor\u001b[39;00m torch\u001b[39m.\u001b[39misinf(tr_loss_step))\n\u001b[0;32m   1843\u001b[0m ):\n\u001b[0;32m   1844\u001b[0m     \u001b[39m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[0;32m   1845\u001b[0m     tr_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m tr_loss \u001b[39m/\u001b[39m (\u001b[39m1\u001b[39m \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mglobal_step \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_globalstep_last_logged)\n","File \u001b[1;32mc:\\Users\\Muajeh Lee\\anaconda3\\Lib\\site-packages\\transformers\\trainer.py:2693\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[1;34m(self, model, inputs)\u001b[0m\n\u001b[0;32m   2691\u001b[0m         scaled_loss\u001b[39m.\u001b[39mbackward()\n\u001b[0;32m   2692\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 2693\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maccelerator\u001b[39m.\u001b[39mbackward(loss)\n\u001b[0;32m   2695\u001b[0m \u001b[39mreturn\u001b[39;00m loss\u001b[39m.\u001b[39mdetach() \u001b[39m/\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mgradient_accumulation_steps\n","File \u001b[1;32mc:\\Users\\Muajeh Lee\\anaconda3\\Lib\\site-packages\\accelerate\\accelerator.py:1905\u001b[0m, in \u001b[0;36mAccelerator.backward\u001b[1;34m(self, loss, **kwargs)\u001b[0m\n\u001b[0;32m   1903\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscaler\u001b[39m.\u001b[39mscale(loss)\u001b[39m.\u001b[39mbackward(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1904\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1905\u001b[0m     loss\u001b[39m.\u001b[39mbackward(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n","File \u001b[1;32mc:\\Users\\Muajeh Lee\\anaconda3\\Lib\\site-packages\\torch\\_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    512\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    513\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    514\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    515\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    520\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[0;32m    521\u001b[0m     )\n\u001b[1;32m--> 522\u001b[0m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mbackward(\n\u001b[0;32m    523\u001b[0m     \u001b[39mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39minputs\n\u001b[0;32m    524\u001b[0m )\n","File \u001b[1;32mc:\\Users\\Muajeh Lee\\anaconda3\\Lib\\site-packages\\torch\\autograd\\__init__.py:259\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    250\u001b[0m inputs \u001b[39m=\u001b[39m (\n\u001b[0;32m    251\u001b[0m     (inputs,)\n\u001b[0;32m    252\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(inputs, (torch\u001b[39m.\u001b[39mTensor, graph\u001b[39m.\u001b[39mGradientEdge))\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    255\u001b[0m     \u001b[39melse\u001b[39;00m \u001b[39mtuple\u001b[39m()\n\u001b[0;32m    256\u001b[0m )\n\u001b[0;32m    258\u001b[0m grad_tensors_ \u001b[39m=\u001b[39m _tensor_or_tensors_to_tuple(grad_tensors, \u001b[39mlen\u001b[39m(tensors))\n\u001b[1;32m--> 259\u001b[0m grad_tensors_ \u001b[39m=\u001b[39m _make_grads(tensors, grad_tensors_, is_grads_batched\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m    260\u001b[0m \u001b[39mif\u001b[39;00m retain_graph \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    261\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n","File \u001b[1;32mc:\\Users\\Muajeh Lee\\anaconda3\\Lib\\site-packages\\torch\\autograd\\__init__.py:142\u001b[0m, in \u001b[0;36m_make_grads\u001b[1;34m(outputs, grads, is_grads_batched)\u001b[0m\n\u001b[0;32m    136\u001b[0m         msg \u001b[39m=\u001b[39m (\n\u001b[0;32m    137\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mgrad can be implicitly created only for real scalar outputs\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    138\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m but got \u001b[39m\u001b[39m{\u001b[39;00mout\u001b[39m.\u001b[39mdtype\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    139\u001b[0m         )\n\u001b[0;32m    140\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(msg)\n\u001b[0;32m    141\u001b[0m     new_grads\u001b[39m.\u001b[39mappend(\n\u001b[1;32m--> 142\u001b[0m         torch\u001b[39m.\u001b[39mones_like(out, memory_format\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mpreserve_format)\n\u001b[0;32m    143\u001b[0m     )\n\u001b[0;32m    144\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    145\u001b[0m     new_grads\u001b[39m.\u001b[39mappend(\u001b[39mNone\u001b[39;00m)\n","\u001b[1;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}],"source":["torch.set_grad_enabled(True)\n","trainer.train()\n","trainer.evaluate()"]}],"metadata":{"kernelspec":{"display_name":"Python 3.9.12 ('base')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.6"},"orig_nbformat":4,"vscode":{"interpreter":{"hash":"bb3398f4b21c7b026dd5874af3f954bf25f1e8ff81e25d82a94abcbbaacf760b"}}},"nbformat":4,"nbformat_minor":2}
